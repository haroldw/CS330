{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS330_Homework2_Stencil_TF2.0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvkoC8rAYBE7"
      },
      "source": [
        "\n",
        "##Setup\n",
        "\n",
        "You will need to make a copy of this Colab notebook in your Google Drive before you can edit the homework files. You can do so with **File &rarr; Save a copy in Drive**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBkP5aBdfFkd"
      },
      "source": [
        "import os\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "# Need to download the Omniglot dataset -- DON'T MODIFY THIS CELL\n",
        "if not os.path.isdir('./omniglot_resized'):\n",
        "    gdd.download_file_from_google_drive(file_id='1iaSFXIYC3AB8q9K_M-oVMa4pmB7yKMtI',\n",
        "                                        dest_path='./omniglot_resized.zip',\n",
        "                                        unzip=True)\n",
        "\n",
        "assert os.path.isdir('./omniglot_resized')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMtiYUiwI-1K"
      },
      "source": [
        "\"\"\" Utility functions. \"\"\"\n",
        "## NOTE: You do not need to modify this block but you will need to use it.\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "## Loss utilities\n",
        "def cross_entropy_loss(pred, label, k_shot):\n",
        "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=tf.stop_gradient(label)) / k_shot)\n",
        "\n",
        "def accuracy(labels, predictions):\n",
        "  return tf.reduce_mean(tf.cast(tf.equal(labels, predictions), dtype=tf.float32))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_dTnU8JwWWc"
      },
      "source": [
        "\"\"\"Convolutional layers used by MAML model.\"\"\"\n",
        "## NOTE: You do not need to modify this block but you will need to use it.\n",
        "seed = 123\n",
        "def conv_block(inp, cweight, bweight, bn, activation=tf.nn.relu, residual=False):\n",
        "  \"\"\" Perform, conv, batch norm, nonlinearity, and max pool \"\"\"\n",
        "  stride, no_stride = [1,2,2,1], [1,1,1,1]\n",
        "\n",
        "  conv_output = tf.nn.conv2d(input=inp, filters=cweight, strides=no_stride, padding='SAME') + bweight\n",
        "  normed = bn(conv_output)\n",
        "  normed = activation(normed)\n",
        "  return normed\n",
        "\n",
        "class ConvLayers(tf.keras.layers.Layer):\n",
        "  def __init__(self, channels, dim_hidden, dim_output, img_size):\n",
        "    super(ConvLayers, self).__init__()\n",
        "    self.channels = channels\n",
        "    self.dim_hidden = dim_hidden\n",
        "    self.dim_output = dim_output\n",
        "    self.img_size = img_size\n",
        "\n",
        "    weights = {}\n",
        "\n",
        "    dtype = tf.float32\n",
        "    weight_initializer =  tf.keras.initializers.GlorotUniform()\n",
        "    k = 3\n",
        "\n",
        "    weights['conv1'] = tf.Variable(weight_initializer(shape=[k, k, self.channels, self.dim_hidden]), name='conv1', dtype=dtype)\n",
        "    weights['b1'] = tf.Variable(tf.zeros([self.dim_hidden]), name='b1')\n",
        "    self.bn1 = tf.keras.layers.BatchNormalization(name='bn1')\n",
        "    weights['conv2'] = tf.Variable(weight_initializer(shape=[k, k, self.dim_hidden, self.dim_hidden]), name='conv2', dtype=dtype)\n",
        "    weights['b2'] = tf.Variable(tf.zeros([self.dim_hidden]), name='b2')\n",
        "    self.bn2 = tf.keras.layers.BatchNormalization(name='bn2')\n",
        "    weights['conv3'] = tf.Variable(weight_initializer(shape=[k, k, self.dim_hidden, self.dim_hidden]), name='conv3', dtype=dtype)\n",
        "    weights['b3'] = tf.Variable(tf.zeros([self.dim_hidden]), name='b3')\n",
        "    self.bn3 = tf.keras.layers.BatchNormalization(name='bn3')\n",
        "    weights['conv4'] = tf.Variable(weight_initializer([k, k, self.dim_hidden, self.dim_hidden]), name='conv4', dtype=dtype)\n",
        "    weights['b4'] = tf.Variable(tf.zeros([self.dim_hidden]), name='b4')\n",
        "    self.bn4 = tf.keras.layers.BatchNormalization(name='bn4')\n",
        "    weights['w5'] = tf.Variable(weight_initializer(shape=[self.dim_hidden, self.dim_output]), name='w5', dtype=dtype)\n",
        "    weights['b5'] = tf.Variable(tf.zeros([self.dim_output]), name='b5')\n",
        "    self.conv_weights = weights\n",
        "\n",
        "  def call(self, inp, weights):\n",
        "    channels = self.channels\n",
        "    inp = tf.reshape(inp, [-1, self.img_size, self.img_size, channels])\n",
        "    hidden1 = conv_block(inp, weights['conv1'], weights['b1'], self.bn1)\n",
        "    hidden2 = conv_block(hidden1, weights['conv2'], weights['b2'], self.bn2)\n",
        "    hidden3 = conv_block(hidden2, weights['conv3'], weights['b3'], self.bn3)\n",
        "    hidden4 = conv_block(hidden3, weights['conv4'], weights['b4'], self.bn4)\n",
        "    hidden4 = tf.reduce_mean(input_tensor=hidden4, axis=[1, 2])\n",
        "    return tf.matmul(hidden4, weights['w5']) + weights['b5']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXZS_JULriBh"
      },
      "source": [
        "\"\"\"Data loading scripts\"\"\"\n",
        "## NOTE: You do not need to modify this block but you will need to use it.\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from scipy import misc\n",
        "import imageio\n",
        "\n",
        "def get_images(paths, labels, n_samples=None, shuffle=True):\n",
        "  \"\"\"\n",
        "  Takes a set of character folders and labels and returns paths to image files\n",
        "  paired with labels.\n",
        "  Args:\n",
        "    paths: A list of character folders\n",
        "    labels: List or numpy array of same length as paths\n",
        "    n_samples: Number of images to retrieve per character\n",
        "  Returns:\n",
        "    List of (label, image_path) tuples\n",
        "  \"\"\"\n",
        "  if n_samples is not None:\n",
        "    sampler = lambda x: random.sample(x, n_samples)\n",
        "  else:\n",
        "    sampler = lambda x: x\n",
        "  images_labels = [(i, os.path.join(path, image))\n",
        "           for i, path in zip(labels, paths)\n",
        "           for image in sampler(os.listdir(path))]\n",
        "  if shuffle:\n",
        "    random.shuffle(images_labels)\n",
        "  return images_labels\n",
        "\n",
        "\n",
        "def image_file_to_array(filename, dim_input):\n",
        "  \"\"\"\n",
        "  Takes an image path and returns numpy array\n",
        "  Args:\n",
        "    filename: Image filename\n",
        "    dim_input: Flattened shape of image\n",
        "  Returns:\n",
        "    1 channel image\n",
        "  \"\"\"\n",
        "  image = imageio.imread(filename)\n",
        "  image = image.reshape([dim_input])\n",
        "  image = image.astype(np.float32) / 255.0\n",
        "  image = 1.0 - image\n",
        "  return image\n",
        "\n",
        "\n",
        "class DataGenerator(object):\n",
        "  \"\"\"\n",
        "  Data Generator capable of generating batches of Omniglot data.\n",
        "  A \"class\" is considered a class of omniglot digits.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_classes, num_samples_per_class, num_meta_test_classes, num_meta_test_samples_per_class, config={}):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      num_classes: Number of classes for classification (K-way)\n",
        "      num_samples_per_class: num samples to generate per class in one batch\n",
        "      num_meta_test_classes: Number of classes for classification (K-way) at meta-test time\n",
        "      num_meta_test_samples_per_class: num samples to generate per class in one batch at meta-test time\n",
        "      batch_size: size of meta batch size (e.g. number of functions)\n",
        "    \"\"\"\n",
        "    self.num_samples_per_class = num_samples_per_class\n",
        "    self.num_classes = num_classes\n",
        "    self.num_meta_test_samples_per_class = num_meta_test_samples_per_class\n",
        "    self.num_meta_test_classes = num_meta_test_classes\n",
        "\n",
        "    data_folder = config.get('data_folder', './omniglot_resized')\n",
        "    self.img_size = config.get('img_size', (28, 28))\n",
        "\n",
        "    self.dim_input = np.prod(self.img_size)\n",
        "    self.dim_output = self.num_classes\n",
        "\n",
        "    character_folders = [os.path.join(data_folder, family, character)\n",
        "               for family in os.listdir(data_folder)\n",
        "               if os.path.isdir(os.path.join(data_folder, family))\n",
        "               for character in os.listdir(os.path.join(data_folder, family))\n",
        "               if os.path.isdir(os.path.join(data_folder, family, character))]\n",
        "\n",
        "    random.seed(123)\n",
        "    random.shuffle(character_folders)\n",
        "    num_val = 100\n",
        "    num_train = 1100\n",
        "    self.metatrain_character_folders = character_folders[: num_train]\n",
        "    self.metaval_character_folders = character_folders[\n",
        "      num_train:num_train + num_val]\n",
        "    self.metatest_character_folders = character_folders[\n",
        "      num_train + num_val:]\n",
        "\n",
        "  def sample_batch(self, batch_type, batch_size, shuffle=True, swap=False):\n",
        "    \"\"\"\n",
        "    Samples a batch for training, validation, or testing\n",
        "    Args:\n",
        "      batch_type: meta_train/meta_val/meta_test\n",
        "      shuffle: randomly shuffle classes or not\n",
        "      swap: swap number of classes (N) and number of samples per class (K) or not\n",
        "    Returns:\n",
        "      A a tuple of (1) Image batch and (2) Label batch where\n",
        "      image batch has shape [B, N, K, 784] and label batch has shape [B, N, K, N] if swap is False\n",
        "      where B is batch size, K is number of samples per class, N is number of classes\n",
        "    \"\"\"\n",
        "    if batch_type == \"meta_train\":\n",
        "      folders = self.metatrain_character_folders\n",
        "      num_classes = self.num_classes\n",
        "      num_samples_per_class = self.num_samples_per_class\n",
        "    elif batch_type == \"meta_val\":\n",
        "      folders = self.metaval_character_folders\n",
        "      num_classes = self.num_classes\n",
        "      num_samples_per_class = self.num_samples_per_class\n",
        "    else:\n",
        "      folders = self.metatest_character_folders\n",
        "      num_classes = self.num_meta_test_classes\n",
        "      num_samples_per_class = self.num_meta_test_samples_per_class\n",
        "    all_image_batches, all_label_batches = [], []\n",
        "    for i in range(batch_size):\n",
        "      sampled_character_folders = random.sample(\n",
        "        folders, num_classes)\n",
        "      labels_and_images = get_images(sampled_character_folders, range(\n",
        "        num_classes), n_samples=num_samples_per_class, shuffle=False)\n",
        "      labels = [li[0] for li in labels_and_images]\n",
        "      images = [image_file_to_array(\n",
        "        li[1], self.dim_input) for li in labels_and_images]\n",
        "      images = np.stack(images)\n",
        "      labels = np.array(labels).astype(np.int32)\n",
        "      labels = np.reshape(\n",
        "        labels, (num_classes, num_samples_per_class))\n",
        "      labels = np.eye(num_classes, dtype=np.float32)[labels]\n",
        "      images = np.reshape(\n",
        "        images, (num_classes, num_samples_per_class, -1))\n",
        "\n",
        "      batch = np.concatenate([labels, images], 2)\n",
        "      if shuffle:\n",
        "        for p in range(num_samples_per_class):\n",
        "          np.random.shuffle(batch[:, p])\n",
        "\n",
        "      labels = batch[:, :, :num_classes]\n",
        "      images = batch[:, :, num_classes:]\n",
        "\n",
        "      if swap:\n",
        "        labels = np.swapaxes(labels, 0, 1)\n",
        "        images = np.swapaxes(images, 0, 1)\n",
        "\n",
        "      all_image_batches.append(images)\n",
        "      all_label_batches.append(labels)\n",
        "    all_image_batches = np.stack(all_image_batches)\n",
        "    all_label_batches = np.stack(all_label_batches)\n",
        "    return all_image_batches, all_label_batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxriXFvwsGfp"
      },
      "source": [
        "\"\"\"MAML model code\"\"\"\n",
        "import numpy as np\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "class MAML(tf.keras.Model):\n",
        "  def __init__(self, dim_input=1, dim_output=1,\n",
        "               num_inner_updates=1,\n",
        "               inner_update_lr=0.4, num_filters=32, k_shot=5, learn_inner_update_lr=False):\n",
        "    super(MAML, self).__init__()\n",
        "    self.dim_input = dim_input\n",
        "    self.dim_output = dim_output\n",
        "    self.inner_update_lr = inner_update_lr\n",
        "    self.loss_func = partial(cross_entropy_loss, k_shot=k_shot)\n",
        "    self.dim_hidden = num_filters\n",
        "    self.channels = 1\n",
        "    self.img_size = int(np.sqrt(self.dim_input/self.channels))\n",
        "\n",
        "    # outputs_ts[i] and losses_ts_post[i] are the output and loss after i+1 inner gradient updates\n",
        "    losses_tr_pre, outputs_tr, losses_ts_post, outputs_ts = [], [], [], []\n",
        "    accuracies_tr_pre, accuracies_ts = [], []\n",
        "\n",
        "    # for each loop in the inner training loop\n",
        "    outputs_ts = [[]]*num_inner_updates\n",
        "    losses_ts_post = [[]]*num_inner_updates\n",
        "    accuracies_ts = [[]]*num_inner_updates\n",
        "\n",
        "    # Define the weights - these should NOT be directly modified by the\n",
        "    # inner training loop\n",
        "    tf.random.set_seed(seed)\n",
        "    self.conv_layers = ConvLayers(self.channels, self.dim_hidden, self.dim_output, self.img_size)\n",
        "\n",
        "    self.learn_inner_update_lr = learn_inner_update_lr\n",
        "    if self.learn_inner_update_lr:\n",
        "      self.inner_update_lr_dict = {}\n",
        "      for key in self.conv_layers.conv_weights.keys():\n",
        "        self.inner_update_lr_dict[key] = [tf.Variable(self.inner_update_lr, name='inner_update_lr_%s_%d' % (key, j)) for j in range(num_inner_updates)]\n",
        "  \n",
        "\n",
        "  def call(self, inp, meta_batch_size=25, num_inner_updates=1):\n",
        "    def task_inner_loop(inp, reuse=True,\n",
        "                      meta_batch_size=25, num_inner_updates=1):\n",
        "      \"\"\"\n",
        "        Perform gradient descent for one task in the meta-batch (i.e. inner-loop).\n",
        "        Args:\n",
        "          inp: a tuple (input_tr, input_ts, label_tr, label_ts), where input_tr and label_tr are the inputs and\n",
        "            labels used for calculating inner loop gradients and input_ts and label_ts are the inputs and\n",
        "            labels used for evaluating the model after inner updates.\n",
        "            Should be shapes:\n",
        "              input_tr: [N*K, 784]\n",
        "              input_ts: [N*K, 784]\n",
        "              label_tr: [N*K, N]\n",
        "              label_ts: [N*K, N]\n",
        "        Returns:\n",
        "          task_output: a list of outputs, losses and accuracies at each inner update\n",
        "      \"\"\"\n",
        "      # the inner and outer loop data\n",
        "      input_tr, input_ts, label_tr, label_ts = inp\n",
        "\n",
        "      # weights corresponds to the initial weights in MAML (i.e. the meta-parameters)\n",
        "      weights = self.conv_layers.conv_weights\n",
        "\n",
        "      # the predicted outputs, loss values, and accuracy for the pre-update model (with the initial weights)\n",
        "      # evaluated on the inner loop training data\n",
        "      task_output_tr_pre, task_loss_tr_pre, task_accuracy_tr_pre = None, None, None\n",
        "\n",
        "      # lists to keep track of outputs, losses, and accuracies of test data for each inner_update\n",
        "      # where task_outputs_ts[i], task_losses_ts[i], task_accuracies_ts[i] are the output, loss, and accuracy\n",
        "      # after i+1 inner gradient updates\n",
        "      task_outputs_ts, task_losses_ts, task_accuracies_ts = [], [], []\n",
        "  \n",
        "      #############################\n",
        "      #### YOUR CODE GOES HERE ####\n",
        "      # perform num_inner_updates to get modified weights\n",
        "      # modified weights should be used to evaluate performance\n",
        "      # Note that at each inner update, always use input_tr and label_tr for calculating gradients\n",
        "      # and use input_ts and labels for evaluating performance\n",
        "\n",
        "      # HINTS: You will need to use tf.GradientTape().\n",
        "      # Read through the tf.GradientTape() documentation to see how 'persistent' should be set.\n",
        "      # Here is some documentation that may be useful: \n",
        "      # https://www.tensorflow.org/guide/advanced_autodiff#higher-order_gradients\n",
        "      # https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
        "\n",
        "      \n",
        "      #############################\n",
        "\n",
        "      # Compute accuracies from output predictions\n",
        "      task_accuracy_tr_pre = accuracy(tf.argmax(input=label_tr, axis=1), tf.argmax(input=tf.nn.softmax(task_output_tr_pre), axis=1))\n",
        "\n",
        "      for j in range(num_inner_updates):\n",
        "        task_accuracies_ts.append(accuracy(tf.argmax(input=label_ts, axis=1), tf.argmax(input=tf.nn.softmax(task_outputs_ts[j]), axis=1)))\n",
        "\n",
        "      task_output = [task_output_tr_pre, task_outputs_ts, task_loss_tr_pre, task_losses_ts, task_accuracy_tr_pre, task_accuracies_ts]\n",
        "\n",
        "      return task_output\n",
        "\n",
        "    input_tr, input_ts, label_tr, label_ts = inp\n",
        "    # to initialize the batch norm vars, might want to combine this, and not run idx 0 twice.\n",
        "    unused = task_inner_loop((input_tr[0], input_ts[0], label_tr[0], label_ts[0]),\n",
        "                          False,\n",
        "                          meta_batch_size,\n",
        "                          num_inner_updates)\n",
        "    out_dtype = [tf.float32, [tf.float32]*num_inner_updates, tf.float32, [tf.float32]*num_inner_updates]\n",
        "    out_dtype.extend([tf.float32, [tf.float32]*num_inner_updates])\n",
        "    task_inner_loop_partial = partial(task_inner_loop, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "    result = tf.map_fn(task_inner_loop_partial,\n",
        "                    elems=(input_tr, input_ts, label_tr, label_ts),\n",
        "                    dtype=out_dtype,\n",
        "                    parallel_iterations=meta_batch_size)\n",
        "    return result\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy1pz_ousUsz"
      },
      "source": [
        "\"\"\"Model training code\"\"\"\n",
        "\"\"\"\n",
        "Usage Instructions:\n",
        "  5-way, 1-shot omniglot:\n",
        "    python main.py --meta_train_iterations=15000 --meta_batch_size=25 --k_shot=1 --inner_update_lr=0.4 --num_inner_updates=1 --logdir=logs/omniglot5way/\n",
        "  20-way, 1-shot omniglot:\n",
        "    python main.py --meta_train_iterations=15000 --meta_batch_size=16 --k_shot=1 --n_way=20 --inner_update_lr=0.1 --num_inner_updates=5 --logdir=logs/omniglot20way/\n",
        "  To run evaluation, use the '--meta_train=False' flag and the '--meta_test_set=True' flag to use the meta-test set.\n",
        "\"\"\"\n",
        "import csv\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "def outer_train_step(inp, model, optim, meta_batch_size=25, num_inner_updates=1):\n",
        "  with tf.GradientTape(persistent=False) as outer_tape:\n",
        "    result = model(inp, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "    outputs_tr, outputs_ts, losses_tr_pre, losses_ts, accuracies_tr_pre, accuracies_ts = result\n",
        "\n",
        "    total_losses_ts = [tf.reduce_mean(loss_ts) for loss_ts in losses_ts]\n",
        "\n",
        "  gradients = outer_tape.gradient(total_losses_ts[-1], model.trainable_variables)\n",
        "  optim.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  total_loss_tr_pre = tf.reduce_mean(losses_tr_pre)\n",
        "  total_accuracy_tr_pre = tf.reduce_mean(accuracies_tr_pre)\n",
        "  total_accuracies_ts = [tf.reduce_mean(accuracy_ts) for accuracy_ts in accuracies_ts]\n",
        "\n",
        "  return outputs_tr, outputs_ts, total_loss_tr_pre, total_losses_ts, total_accuracy_tr_pre, total_accuracies_ts\n",
        "\n",
        "def outer_eval_step(inp, model, meta_batch_size=25, num_inner_updates=1):\n",
        "  result = model(inp, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "  outputs_tr, outputs_ts, losses_tr_pre, losses_ts, accuracies_tr_pre, accuracies_ts = result\n",
        "\n",
        "  total_loss_tr_pre = tf.reduce_mean(losses_tr_pre)\n",
        "  total_losses_ts = [tf.reduce_mean(loss_ts) for loss_ts in losses_ts]\n",
        "\n",
        "  total_accuracy_tr_pre = tf.reduce_mean(accuracies_tr_pre)\n",
        "  total_accuracies_ts = [tf.reduce_mean(accuracy_ts) for accuracy_ts in accuracies_ts]\n",
        "\n",
        "  return outputs_tr, outputs_ts, total_loss_tr_pre, total_losses_ts, total_accuracy_tr_pre, total_accuracies_ts  \n",
        "\n",
        "\n",
        "def meta_train_fn(model, exp_string, data_generator,\n",
        "               n_way=5, meta_train_iterations=15000, meta_batch_size=25,\n",
        "               log=True, logdir='/tmp/data', k_shot=1, num_inner_updates=1, meta_lr=0.001):\n",
        "  SUMMARY_INTERVAL = 10\n",
        "  SAVE_INTERVAL = 100\n",
        "  PRINT_INTERVAL = 10  \n",
        "  TEST_PRINT_INTERVAL = PRINT_INTERVAL*5\n",
        "\n",
        "  pre_accuracies, post_accuracies = [], []\n",
        "\n",
        "  num_classes = data_generator.num_classes\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=meta_lr)\n",
        "\n",
        "  for itr in range(meta_train_iterations):\n",
        "    #############################\n",
        "    #### YOUR CODE GOES HERE ####\n",
        "\n",
        "    # sample a batch of training data and partition into\n",
        "    # the support/training set (input_tr, label_tr) and the query/test set (input_ts, label_ts)\n",
        "    # NOTE: The code assumes that the support and query sets have the same number of examples.\n",
        "\n",
        "    #############################\n",
        "\n",
        "    inp = (input_tr, input_ts, label_tr, label_ts)\n",
        "    \n",
        "    result = outer_train_step(inp, model, optimizer, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "    if itr % SUMMARY_INTERVAL == 0:\n",
        "      pre_accuracies.append(result[-2])\n",
        "      post_accuracies.append(result[-1][-1])\n",
        "\n",
        "    if (itr!=0) and itr % PRINT_INTERVAL == 0:\n",
        "      print_str = 'Iteration %d: pre-inner-loop train accuracy: %.5f, post-inner-loop test accuracy: %.5f' % (itr, np.mean(pre_accuracies), np.mean(post_accuracies))\n",
        "      print(print_str)\n",
        "      pre_accuracies, post_accuracies = [], []\n",
        "\n",
        "    if (itr!=0) and itr % TEST_PRINT_INTERVAL == 0:\n",
        "      #############################\n",
        "      #### YOUR CODE GOES HERE ####\n",
        "\n",
        "      # sample a batch of validation data and partition it into\n",
        "      # the support/training set (input_tr, label_tr) and the query/test set (input_ts, label_ts)\n",
        "      # NOTE: The code assumes that the support and query sets have the same number of examples.\n",
        "\n",
        "      #############################\n",
        "\n",
        "      inp = (input_tr, input_ts, label_tr, label_ts)\n",
        "      result = outer_eval_step(inp, model, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "      print('Meta-validation pre-inner-loop train accuracy: %.5f, meta-validation post-inner-loop test accuracy: %.5f' % (result[-2], result[-1][-1]))\n",
        "\n",
        "  model_file = logdir + '/' + exp_string +  '/model' + str(itr)\n",
        "  print(\"Saving to \", model_file)\n",
        "  model.save_weights(model_file)\n",
        "\n",
        "# calculated for omniglot\n",
        "NUM_META_TEST_POINTS = 600\n",
        "\n",
        "def meta_test_fn(model, data_generator, n_way=5, meta_batch_size=25, k_shot=1,\n",
        "              num_inner_updates=1):\n",
        "  \n",
        "  num_classes = data_generator.num_classes\n",
        "\n",
        "  np.random.seed(1)\n",
        "  random.seed(1)\n",
        "\n",
        "  meta_test_accuracies = []\n",
        "\n",
        "  for _ in range(NUM_META_TEST_POINTS):\n",
        "    #############################\n",
        "    #### YOUR CODE GOES HERE ####\n",
        "\n",
        "    # sample a batch of test data and partition it into\n",
        "    # the support/training set (input_tr, label_tr) and the query/test set (input_ts, label_ts)\n",
        "    # NOTE: The code assumes that the support and query sets have the same number of examples.\n",
        "    \n",
        "    #############################\n",
        "    inp = (input_tr, input_ts, label_tr, label_ts)\n",
        "    result = outer_eval_step(inp, model, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "    meta_test_accuracies.append(result[-1][-1])\n",
        "\n",
        "  meta_test_accuracies = np.array(meta_test_accuracies)\n",
        "  means = np.mean(meta_test_accuracies)\n",
        "  stds = np.std(meta_test_accuracies)\n",
        "  ci95 = 1.96*stds/np.sqrt(NUM_META_TEST_POINTS)\n",
        "\n",
        "  print('Mean meta-test accuracy/loss, stddev, and confidence intervals')\n",
        "  print((means, stds, ci95))\n",
        "\n",
        "\n",
        "def run_maml(n_way=5, k_shot=1, meta_batch_size=25, meta_lr=0.001,\n",
        "             inner_update_lr=0.4, num_filters=32, num_inner_updates=1,\n",
        "             learn_inner_update_lr=False,\n",
        "             resume=False, resume_itr=0, log=True, logdir='/tmp/data',\n",
        "             data_path='./omniglot_resized',meta_train=True,\n",
        "             meta_train_iterations=15000, meta_train_k_shot=-1,\n",
        "             meta_train_inner_update_lr=-1):\n",
        "\n",
        "\n",
        "  # call data_generator and get data with k_shot*2 samples per class\n",
        "  data_generator = DataGenerator(n_way, k_shot*2, n_way, k_shot*2, config={'data_folder': data_path})\n",
        "\n",
        "  # set up MAML model\n",
        "  dim_output = data_generator.dim_output\n",
        "  dim_input = data_generator.dim_input\n",
        "  model = MAML(dim_input,\n",
        "              dim_output,\n",
        "              num_inner_updates=num_inner_updates,\n",
        "              inner_update_lr=inner_update_lr,\n",
        "              k_shot=k_shot,\n",
        "              num_filters=num_filters,\n",
        "              learn_inner_update_lr=learn_inner_update_lr)\n",
        "\n",
        "  if meta_train_k_shot == -1:\n",
        "    meta_train_k_shot = k_shot\n",
        "  if meta_train_inner_update_lr == -1:\n",
        "    meta_train_inner_update_lr = inner_update_lr\n",
        "\n",
        "  exp_string = 'cls_'+str(n_way)+'.mbs_'+str(meta_batch_size) + '.k_shot_' + str(meta_train_k_shot) + '.inner_numstep_' + str(num_inner_updates) + '.inner_updatelr_' + str(meta_train_inner_update_lr) + '.learn_inner_update_lr_' + str(learn_inner_update_lr)\n",
        "\n",
        "\n",
        "  if meta_train:\n",
        "    meta_train_fn(model, exp_string, data_generator,\n",
        "                  n_way, meta_train_iterations, meta_batch_size, log, logdir,\n",
        "                  k_shot, num_inner_updates, meta_lr)\n",
        "  else:\n",
        "    meta_batch_size = 1\n",
        "\n",
        "    model_file = tf.train.latest_checkpoint(logdir + '/' + exp_string)\n",
        "    print(\"Restoring model weights from \", model_file)\n",
        "    model.load_weights(model_file)\n",
        "\n",
        "    meta_test_fn(model, data_generator, n_way, meta_batch_size, k_shot, num_inner_updates)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USOh7VulTMK3"
      },
      "source": [
        "run_maml(n_way=5, k_shot=1, inner_update_lr=0.4, num_inner_updates=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohmGfgV-geFj"
      },
      "source": [
        "# models/ProtoNet\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class ProtoNet(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, num_filters, latent_dim):\n",
        "    super(ProtoNet, self).__init__()\n",
        "    self.num_filters = num_filters\n",
        "    self.latent_dim = latent_dim\n",
        "    num_filter_list = self.num_filters + [latent_dim]\n",
        "    self.convs = []\n",
        "    for i, num_filter in enumerate(num_filter_list):\n",
        "      block_parts = [\n",
        "        layers.Conv2D(\n",
        "          filters=num_filter,\n",
        "          kernel_size=3,\n",
        "          padding='SAME',\n",
        "          activation='linear'),\n",
        "      ]\n",
        "\n",
        "      block_parts += [layers.BatchNormalization()]\n",
        "      block_parts += [layers.Activation('relu')]\n",
        "      block_parts += [layers.MaxPool2D()]\n",
        "      block = tf.keras.Sequential(block_parts, name='conv_block_%d' % i)\n",
        "      self.__setattr__(\"conv%d\" % i, block)\n",
        "      self.convs.append(block)\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "\n",
        "  def call(self, inp):\n",
        "    out = inp\n",
        "    for conv in self.convs:\n",
        "      out = conv(out)\n",
        "    out = self.flatten(out)\n",
        "    return out\n",
        "\n",
        "def ProtoLoss(x_latent, q_latent, labels_onehot, num_classes, num_support, num_queries):\n",
        "  \"\"\"\n",
        "    calculates the prototype network loss using the latent representation of x\n",
        "    and the latent representation of the query set\n",
        "    Args:\n",
        "      x_latent: latent representation of supports with shape [N*S, D], where D is the latent dimension\n",
        "      q_latent: latent representation of queries with shape [N*Q, D], where D is the latent dimension\n",
        "      labels_onehot: one-hot encodings of the labels of the queries with shape [N, Q, N]\n",
        "      num_classes: number of classes (N) for classification\n",
        "      num_support: number of examples (S) in the support set\n",
        "      num_queries: number of examples (Q) in the query set\n",
        "    Returns:\n",
        "      ce_loss: the cross entropy loss between the predicted labels and true labels\n",
        "      acc: the accuracy of classification on the queries\n",
        "  \"\"\"\n",
        "  #############################\n",
        "  #### YOUR CODE GOES HERE ####\n",
        "\n",
        "  # compute the prototypes\n",
        "  # compute the distance from the prototypes\n",
        "  # compute cross entropy loss\n",
        "  # note - additional steps are needed!\n",
        "  # return the cross-entropy loss and accuracy\n",
        "\n",
        "  #############################\n",
        "  return ce_loss, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_bOml4PhkSM"
      },
      "source": [
        "# run_ProtoNet\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def proto_net_train_step(model, optim, x, q, labels_ph):\n",
        "  num_classes, num_support, im_height, im_width, channels = x.shape\n",
        "  num_queries = q.shape[1]\n",
        "  x = tf.reshape(x, [-1, im_height, im_width, channels])\n",
        "  q = tf.reshape(q, [-1, im_height, im_width, channels])\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    x_latent = model(x)\n",
        "    q_latent = model(q)\n",
        "    ce_loss, acc = ProtoLoss(x_latent, q_latent, labels_ph, num_classes, num_support, num_queries)\n",
        "\n",
        "  gradients = tape.gradient(ce_loss, model.trainable_variables)\n",
        "  optim.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return ce_loss, acc\n",
        "\n",
        "def proto_net_eval(model, x, q, labels_ph):\n",
        "  num_classes, num_support, im_height, im_width, channels = x.shape\n",
        "  num_queries = q.shape[1]\n",
        "  x = tf.reshape(x, [-1, im_height, im_width, channels])\n",
        "  q = tf.reshape(q, [-1, im_height, im_width, channels])\n",
        "\n",
        "  x_latent = model(x)\n",
        "  q_latent = model(q)\n",
        "  ce_loss, acc = ProtoLoss(x_latent, q_latent, labels_ph, num_classes, num_support, num_queries)\n",
        "\n",
        "  return ce_loss, acc \n",
        "\n",
        "def run_protonet(data_path='./omniglot_resized', n_way=20, k_shot=1, n_query=5, n_meta_test_way=20, k_meta_test_shot=5, n_meta_test_query=5):\n",
        "  n_epochs = 20\n",
        "  n_episodes = 100\n",
        "\n",
        "  im_width, im_height, channels = 28, 28, 1\n",
        "  num_filters = 32\n",
        "  latent_dim = 16\n",
        "  num_conv_layers = 3\n",
        "  n_meta_test_episodes = 1000\n",
        "\n",
        "  model = ProtoNet([num_filters]*num_conv_layers, latent_dim)\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    # call DataGenerator with k_shot+n_query samples per class\n",
        "  data_generator = DataGenerator(n_way, k_shot+n_query, n_meta_test_way, k_meta_test_shot+n_meta_test_query)\n",
        "  for ep in range(n_epochs):\n",
        "    for epi in range(n_episodes):\n",
        "      #############################\n",
        "      #### YOUR CODE GOES HERE ####\n",
        "\n",
        "      # sample a batch of training data and partition it into\n",
        "      # support and query sets\n",
        "\n",
        "      #############################\n",
        "      ls, ac = proto_net_train_step(model, optimizer, x=support, q=query, labels_ph=labels)\n",
        "      if (epi+1) % 50 == 0:\n",
        "        #############################\n",
        "        #### YOUR CODE GOES HERE ####\n",
        "\n",
        "        # sample a batch of validation data and partition it into\n",
        "        # support and query sets\n",
        "\n",
        "        #############################\n",
        "        val_ls, val_ac = proto_net_eval(model, x=support, q=query, labels_ph=labels)\n",
        "        print('[epoch {}/{}, episode {}/{}] => meta-training loss: {:.5f}, meta-training acc: {:.5f}, meta-val loss: {:.5f}, meta-val acc: {:.5f}'.format(ep+1,\n",
        "                                                                    n_epochs,\n",
        "                                                                    epi+1,\n",
        "                                                                    n_episodes,\n",
        "                                                                    ls,\n",
        "                                                                    ac,\n",
        "                                                                    val_ls,\n",
        "                                                                    val_ac))\n",
        "\n",
        "  print('Testing...')\n",
        "  meta_test_accuracies = []\n",
        "  for epi in range(n_meta_test_episodes):\n",
        "    #############################\n",
        "    #### YOUR CODE GOES HERE ####\n",
        "\n",
        "    # sample a batch of test data and partition it into\n",
        "    # support and query sets\n",
        "\n",
        "    #############################\n",
        "    ls, ac = proto_net_eval(model, x=support, q=query, labels_ph=labels)\n",
        "    meta_test_accuracies.append(ac)\n",
        "    if (epi+1) % 50 == 0:\n",
        "      print('[meta-test episode {}/{}] => loss: {:.5f}, acc: {:.5f}'.format(epi+1, n_meta_test_episodes, ls, ac))\n",
        "  avg_acc = np.mean(meta_test_accuracies)\n",
        "  stds = np.std(meta_test_accuracies)\n",
        "  print('Average Meta-Test Accuracy: {:.5f}, Meta-Test Accuracy Std: {:.5f}'.format(avg_acc, stds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6Tv12fbTQqJ"
      },
      "source": [
        "run_protonet('./omniglot_resized/', n_way=5, k_shot=1, n_query=5, n_meta_test_way=5, k_meta_test_shot=4, n_meta_test_query=4)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}